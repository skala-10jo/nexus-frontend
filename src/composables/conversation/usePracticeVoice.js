/**
 * Practice ìŒì„± ì…ë ¥ Composable
 *
 * íšŒí™” ì—°ìŠµ í˜ì´ì§€ì˜ ìŒì„± ì…ë ¥ ë° ì•„ë°”íƒ€ ê´€ë¦¬ë¥¼ ë‹´ë‹¹í•©ë‹ˆë‹¤.
 * - ìŒì„±/í…ìŠ¤íŠ¸ ì…ë ¥ ëª¨ë“œ ì „í™˜
 * - STT (Speech-to-Text) ë…¹ìŒ
 * - ì•„ë°”íƒ€ ëª¨ë“œ ê´€ë¦¬
 *
 * @module usePracticeVoice
 */
import { ref, computed } from 'vue'
import { useRealtimeSTT } from '@/composables/useRealtimeSTT'

/**
 * Practice ìŒì„± ì…ë ¥ ë¡œì§
 *
 * @param {Object} options - ì˜µì…˜
 * @param {Ref<string>} options.userInput - ì‚¬ìš©ì ì…ë ¥ ref
 * @param {Function} options.onSendMessage - ë©”ì‹œì§€ ì „ì†¡ ì½œë°±
 * @param {Ref<Object>} options.scenario - ì‹œë‚˜ë¦¬ì˜¤ ref (ì–¸ì–´ ì •ë³´ í¬í•¨)
 * @returns {Object} ìŒì„± ì…ë ¥ ìƒíƒœ ë° ë©”ì„œë“œ
 */
export function usePracticeVoice({ userInput, onSendMessage, scenario }) {
  // ============================================
  // Realtime STT Composable
  // ============================================
  const {
    isRecording: sttIsRecording,
    isConnecting: sttIsConnecting,
    isConnected: sttIsConnected,
    error: sttError,
    interimText: sttInterimText,
    finalTexts: sttFinalTexts,
    fullText: sttFullText,
    recordingTime: sttRecordingTime,
    startRecording: startRealtimeSTT,
    stopRecording: stopRealtimeSTT,
    clearResults: clearSTTResults,
    setOnAutoRecognized,
    setTTSPlaying
  } = useRealtimeSTT()

  // ============================================
  // State
  // ============================================
  const inputMode = ref('text') // 'text' | 'voice'
  const isProcessingVoice = ref(false)
  const recognizedText = ref('')
  const lastAudioBlob = ref(null)  // ë§ˆì§€ë§‰ ë…¹ìŒëœ ì˜¤ë””ì˜¤ (ë°œìŒ í‰ê°€ìš©)

  // STT ëª¨ë“œ: 'manual' (ìˆ˜ë™ ì¢…ë£Œ) | 'auto' (ìë™ ë¶„ì ˆ - ì¹¨ë¬µ ê°ì§€)
  const sttMode = ref('manual')

  // Avatar
  const avatarEnabled = ref(false)
  const isAvatarInitializing = ref(false)
  const avatarVideoElement = ref(null)

  // ============================================
  // Computed (STT ìƒíƒœ ë˜í•‘)
  // ============================================
  const isRecording = computed(() => sttIsRecording.value)
  const isConnecting = computed(() => sttIsConnecting.value)
  const interimText = computed(() => sttInterimText.value)
  const finalTexts = computed(() => sttFinalTexts.value)
  const recordingTime = computed(() => sttRecordingTime.value)

  // ============================================
  // Actions
  // ============================================

  /**
   * ì…ë ¥ ëª¨ë“œ ì „í™˜ (í…ìŠ¤íŠ¸ â†” ìŒì„±)
   * í…ìŠ¤íŠ¸ ëª¨ë“œë¡œ ì „í™˜ ì‹œ ì•„ë°”íƒ€ ìë™ ë¹„í™œì„±í™”
   */
  const toggleInputMode = () => {
    const newMode = inputMode.value === 'text' ? 'voice' : 'text'
    inputMode.value = newMode

    // í…ìŠ¤íŠ¸ ëª¨ë“œë¡œ ì „í™˜ ì‹œ ì•„ë°”íƒ€ ë¹„í™œì„±í™”
    if (newMode === 'text' && avatarEnabled.value) {
      avatarEnabled.value = false
    }
  }

  /**
   * ISO 639-1 â†’ BCP-47 ë³€í™˜
   * Azure Speech SDKëŠ” BCP-47 í˜•ì‹ í•„ìš” (en â†’ en-US)
   */
  const toBCP47 = (lang) => {
    const mapping = {
      'en': 'en-US',
      'ko': 'ko-KR',
      'ja': 'ja-JP',
      'zh': 'zh-CN',
      'vi': 'vi-VN',
      'es': 'es-ES',
      'fr': 'fr-FR',
      'de': 'de-DE'
    }
    // ì´ë¯¸ BCP-47 í˜•ì‹ì´ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
    if (lang && lang.includes('-')) {
      return lang
    }
    return mapping[lang] || 'en-US'
  }

  /**
   * STT ëª¨ë“œ í† ê¸€ (ìˆ˜ë™ â†” ìë™)
   */
  const toggleSTTMode = () => {
    sttMode.value = sttMode.value === 'manual' ? 'auto' : 'manual'
    console.log(`STT mode changed to: ${sttMode.value}`)
  }

  /**
   * ìë™ ëª¨ë“œì—ì„œ ì¸ì‹ ì™„ë£Œ ì‹œ ë©”ì‹œì§€ ì „ì†¡ (ë…¹ìŒì€ ê³„ì† ìœ ì§€)
   * @param {string} text - ì¸ì‹ëœ í…ìŠ¤íŠ¸
   * @param {Blob|null} audioBlob - í•´ë‹¹ ì„¸ê·¸ë¨¼íŠ¸ì˜ ì˜¤ë””ì˜¤ Blob (ë°œìŒ í‰ê°€ìš©)
   */
  const handleAutoRecognized = async (text, audioBlob = null) => {
    // ì¸ì‹ëœ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥ì— ì„¤ì •
    recognizedText.value = text
    userInput.value = text

    // ì˜¤ë””ì˜¤ ì €ì¥ (ë°œìŒ í‰ê°€ìš©)
    if (audioBlob) {
      lastAudioBlob.value = audioBlob
    }

    // ë…¹ìŒì€ ì¤‘ì§€í•˜ì§€ ì•ŠìŒ! (ìë™ ëª¨ë“œì—ì„œëŠ” ê³„ì† ë…¹ìŒ ìœ ì§€)
    // ì‚¬ìš©ìê°€ Stop ë²„íŠ¼ì„ ëˆ„ë¥¼ ë•Œê¹Œì§€ ê³„ì† ëŒ€í™” ê°€ëŠ¥

    // ë©”ì‹œì§€ ì „ì†¡
    if (onSendMessage) {
      await onSendMessage()
    }

    // ë‹¤ìŒ ë°œí™”ë¥¼ ìœ„í•´ ìƒíƒœ ì´ˆê¸°í™”
    recognizedText.value = ''
    clearSTTResults()
  }

  /**
   * ë…¹ìŒ ì‹œì‘
   * ì‹œë‚˜ë¦¬ì˜¤ì— ì§€ì •ëœ ì–¸ì–´ë¡œ STT ìˆ˜í–‰ (ì–¸ì–´ ê°ì§€ ì—†ì´ ë‹¨ì¼ ì–¸ì–´)
   */
  const startRecording = async () => {
    try {
      recognizedText.value = ''
      clearSTTResults()

      // ì‹œë‚˜ë¦¬ì˜¤ ì–¸ì–´ë¥¼ BCP-47 í˜•ì‹ìœ¼ë¡œ ë³€í™˜
      const rawLang = scenario?.value?.language || 'en'
      const language = toBCP47(rawLang)

      // STT ëª¨ë“œì— ë”°ë¼ autoSegment ì˜µì…˜ ì„¤ì •
      const autoSegment = sttMode.value === 'auto'

      // ìë™ ëª¨ë“œì¼ ë•Œ ì½œë°± ì„¤ì •
      if (autoSegment) {
        setOnAutoRecognized(handleAutoRecognized)
      } else {
        setOnAutoRecognized(null)
      }

      await startRealtimeSTT(language, { autoSegment })
      console.log(`Recording started with language: ${language} (from: ${rawLang}), autoSegment: ${autoSegment}`)
    } catch (err) {
      console.error('Failed to start recording:', err)
      throw new Error(sttError.value || 'ë§ˆì´í¬ ê¶Œí•œì„ í—ˆìš©í•´ì£¼ì„¸ìš”.')
    }
  }

  /**
   * ë…¹ìŒ ì¤‘ì§€ ë° ë©”ì‹œì§€ ì „ì†¡
   */
  const stopRecording = async () => {
    // ìµœì¢… í…ìŠ¤íŠ¸ì™€ ì˜¤ë””ì˜¤ ê°€ì ¸ì˜¤ê¸°
    const result = stopRealtimeSTT()

    // resultê°€ ê°ì²´ì¸ ê²½ìš°ì™€ ë¬¸ìì—´ì¸ ê²½ìš° ëª¨ë‘ ì²˜ë¦¬
    let fullText = ''
    let audioBlob = null

    if (result && typeof result === 'object') {
      fullText = result.text || ''
      audioBlob = result.audioBlob || null
    } else if (typeof result === 'string') {
      fullText = result
    }

    // ì˜¤ë””ì˜¤ ì €ì¥ (ë°œìŒ í‰ê°€ìš©)
    if (audioBlob) {
      lastAudioBlob.value = audioBlob
      console.log('ğŸ™ï¸ Audio blob saved for pronunciation assessment:', audioBlob.size, 'bytes')
    }

    if (fullText && fullText.trim()) {
      recognizedText.value = fullText
      userInput.value = fullText

      // ìë™ ì „ì†¡
      isProcessingVoice.value = true

      setTimeout(async () => {
        isProcessingVoice.value = false

        if (onSendMessage) {
          await onSendMessage()
        }
      }, 500)
    } else {
      recognizedText.value = ''
      lastAudioBlob.value = null  // í…ìŠ¤íŠ¸ ì—†ìœ¼ë©´ ì˜¤ë””ì˜¤ë„ ë²„ë¦¼
    }
  }

  /**
   * ì•„ë°”íƒ€ ëª¨ë“œ í† ê¸€
   */
  const toggleAvatar = () => {
    avatarEnabled.value = !avatarEnabled.value
  }

  // ============================================
  // Return
  // ============================================
  return {
    // State
    inputMode,
    isProcessingVoice,
    recognizedText,
    lastAudioBlob,  // ë§ˆì§€ë§‰ ë…¹ìŒëœ ì˜¤ë””ì˜¤ (ë°œìŒ í‰ê°€ìš©)
    sttMode,        // STT ëª¨ë“œ: 'manual' | 'auto'
    avatarEnabled,
    isAvatarInitializing,
    avatarVideoElement,

    // STT Computed
    isRecording,
    isConnecting,
    interimText,
    finalTexts,
    recordingTime,

    // Actions
    toggleInputMode,
    toggleSTTMode,  // STT ëª¨ë“œ í† ê¸€ (ìˆ˜ë™ â†” ìë™)
    startRecording,
    stopRecording,
    toggleAvatar,
    setTTSPlaying   // TTS ì¬ìƒ ìƒíƒœ ì„¤ì • (ì—ì½” ë°©ì§€ìš©)
  }
}
