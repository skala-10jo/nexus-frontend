/**
 * Multi-language STT Composable
 *
 * ìŒì„±ë²ˆì—­ í˜ì´ì§€ìš© ë‹¤êµ­ì–´ ì‹¤ì‹œê°„ ìŒì„± ì¸ì‹ ë° ë²ˆì—­ ê¸°ëŠ¥
 * ìë™ ì–¸ì–´ ê°ì§€ í›„ ì„ íƒëœ ë‹¤ë¥¸ ì–¸ì–´ë“¤ë¡œ ë²ˆì—­
 *
 * ì „ë¬¸ìš©ì–´ì‚¬ì „ ì§€ì›:
 * - projectIdë¥¼ ì „ë‹¬í•˜ë©´ í•´ë‹¹ í”„ë¡œì íŠ¸ì— ì—°ê²°ëœ ë¬¸ì„œì˜ ìš©ì–´ì§‘ì„ í›„ì²˜ë¦¬ë¡œ ì ìš©
 * - projectIdê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ Azure Translator ë²ˆì—­ë§Œ ìˆ˜í–‰
 *
 * @see createTranslationStream - ë²ˆì—­ í¬í•¨ WebSocket
 * @see voice_realtime.py - ë°±ì—”ë“œ ë²ˆì—­ WebSocket API
 */
import { ref, onUnmounted } from 'vue'
import { createTranslationStream } from '../services/voiceService'

export function useMultiLanguageSTT() {
  const isRecording = ref(false)
  const isConnected = ref(false)
  const error = ref(null)
  const translationCards = ref([])
  const recognizingText = ref('')
  const currentProjectId = ref(null)  // í˜„ì¬ ì„ íƒëœ í”„ë¡œì íŠ¸ ID

  let wsConnection = null
  let audioContext = null
  let audioWorkletNode = null
  let sourceNode = null
  let audioStream = null

  /**
   * ë…¹ìŒ ì‹œì‘
   * @param {string[]} selectedLanguages - ì„ íƒëœ ì–¸ì–´ ëª©ë¡ (BCP-47 ì½”ë“œ)
   * @param {string|null} projectId - í”„ë¡œì íŠ¸ ID (ìš©ì–´ì§‘ ì ìš©, ì„ íƒì‚¬í•­)
   */
  async function startRecording(selectedLanguages = ['ko-KR', 'en-US'], projectId = null) {
    try {
      error.value = null
      currentProjectId.value = projectId

      if (!selectedLanguages || selectedLanguages.length < 2) {
        throw new Error('ìµœì†Œ 2ê°œ ì´ìƒì˜ ì–¸ì–´ë¥¼ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤.')
      }

      // ìš©ì–´ì§‘ ì ìš© ì—¬ë¶€ ë¡œê·¸
      if (projectId) {
        console.log('ğŸ“š [STT] ìš©ì–´ì§‘ ì ìš© ëª¨ë“œ: project_id=' + projectId)
      } else {
        console.log('ğŸ“š [STT] ê¸°ë³¸ ë²ˆì—­ ëª¨ë“œ (ìš©ì–´ì§‘ ë¯¸ì ìš©)')
      }

      // 1. ë§ˆì´í¬ ê¶Œí•œ ìš”ì²­
      audioStream = await navigator.mediaDevices.getUserMedia({
        audio: {
          sampleRate: 48000,
          channelCount: 1,
          echoCancellation: true,
          noiseSuppression: true
        }
      })

      // 2. AudioContext ìƒì„±
      audioContext = new (window.AudioContext || window.webkitAudioContext)()

      // Chrome Autoplay Policy ëŒ€ì‘: suspended ìƒíƒœë©´ resume í•„ìš”
      if (audioContext.state === 'suspended') {
        await audioContext.resume()
      }

      // 3. AudioWorklet ë¡œë“œ
      await audioContext.audioWorklet.addModule('/pcm-processor.js')

      // 4. AudioWorkletNode ìƒì„±
      audioWorkletNode = new AudioWorkletNode(audioContext, 'pcm-processor')

      // 5. ë§ˆì´í¬ ì—°ê²°
      sourceNode = audioContext.createMediaStreamSource(audioStream)
      sourceNode.connect(audioWorkletNode)

      // 6. WebSocket ì—°ê²° (ë‹¤êµ­ì–´ ìë™ ê°ì§€ + ë²ˆì—­ + ìš©ì–´ì§‘ í›„ì²˜ë¦¬)
      wsConnection = createTranslationStream(selectedLanguages, {
        onConnected: () => {
          console.log('âœ… [Translation] WebSocket connected')

          // ì—°ê²° ì¦‰ì‹œ ìƒíƒœ í™•ì¸ (200ms ë”œë ˆì´ ì œê±° - race condition ë°©ì§€)
          if (wsConnection && wsConnection.ws.readyState === WebSocket.OPEN) {
            // PCM ë°ì´í„° ì „ì†¡
            audioWorkletNode.port.onmessage = (event) => {
              if (wsConnection && wsConnection.ws.readyState === WebSocket.OPEN) {
                wsConnection.ws.send(event.data)
              }
            }
            isRecording.value = true
            isConnected.value = true

            console.log('âœ… [Translation] Audio streaming started')
          } else {
            console.warn('âš ï¸ [Translation] WebSocket not in OPEN state after onConnected')
            error.value = 'WebSocket ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.'
            cleanup()
          }
        },

        onRecognizing: (message) => {
          recognizingText.value = message.text || ''
        },

        onRecognized: (message) => {
          recognizingText.value = ''

          if (message.text && message.text.trim()) {
            translationCards.value.push({
              id: Date.now(),
              original: message.text,
              detectedLang: message.detected_language || 'ko-KR',
              translations: message.translations || [],
              timestamp: new Date().toISOString()
            })

            // ìµœëŒ€ 50ê°œ (ì˜¤ë˜ëœ ê²ƒë¶€í„° ì œê±°)
            if (translationCards.value.length > 50) {
              translationCards.value.shift()
            }
          }
        },

        onError: (errorMessage) => {
          error.value = errorMessage
        },

        onEnd: () => {
          isConnected.value = false
        }
      }, projectId)  // í”„ë¡œì íŠ¸ ID ì „ë‹¬ (ìš©ì–´ì§‘ ì ìš©)

    } catch (err) {
      error.value = err.message || 'ë…¹ìŒ ì‹œì‘ ì‹¤íŒ¨'
      cleanup()
      throw err
    }
  }

  function stopRecording() {
    try {
      if (sourceNode) sourceNode.disconnect()
      if (audioWorkletNode) {
        audioWorkletNode.disconnect()
        audioWorkletNode.port.onmessage = null
      }
      if (audioContext && audioContext.state !== 'closed') {
        audioContext.close()
      }
      if (audioStream) {
        audioStream.getTracks().forEach(track => track.stop())
      }
      if (wsConnection) wsConnection.close()

      isRecording.value = false
      isConnected.value = false
      recognizingText.value = ''
    } catch (err) {
      error.value = err.message || 'ë…¹ìŒ ì¤‘ì§€ ì‹¤íŒ¨'
    }
  }

  function clearCards() {
    translationCards.value = []
  }

  function cleanup() {
    if (isRecording.value) stopRecording()
    wsConnection = null
    audioContext = null
    audioWorkletNode = null
    sourceNode = null
    audioStream = null
  }

  onUnmounted(() => cleanup())

  return {
    isRecording,
    isConnected,
    error,
    translationCards,
    recognizingText,
    currentProjectId,  // í˜„ì¬ ì„ íƒëœ í”„ë¡œì íŠ¸ ID
    startRecording,
    stopRecording,
    clearCards
  }
}
