/**
 * Real-time STT Composable for Conversation Practice
 *
 * íšŒí™”ì—°ìŠµ í˜ì´ì§€ìš© ì‹¤ì‹œê°„ ìŒì„± ì¸ì‹ Composable
 * ë²ˆì—­ ì—†ì´ ë‹¨ì¼ ì–¸ì–´ STTì— ì§‘ì¤‘í•˜ì—¬ ë” ë¹ ë¥¸ ì‘ë‹µ ì œê³µ
 *
 * ì£¼ìš” ê¸°ëŠ¥:
 * - ì‹¤ì‹œê°„ ìŒì„± ì¸ì‹ (WebSocket ê¸°ë°˜)
 * - ì¤‘ê°„ ì¸ì‹ ê²°ê³¼ (recognizing) í‘œì‹œ
 * - ìµœì¢… ì¸ì‹ ê²°ê³¼ (recognized) ëˆ„ì 
 * - ë…¹ìŒ ì¤‘ì§€ ì‹œ ì „ì²´ í…ìŠ¤íŠ¸ ë°˜í™˜
 * - ë°œìŒ í‰ê°€ìš© ì˜¤ë””ì˜¤ ìº¡ì²˜
 *
 * @see createSTTOnlyStream - STT ì „ìš© WebSocket (ë²ˆì—­ ì—†ìŒ)
 * @see voice_stt_stream.py - ë°±ì—”ë“œ STT ì „ìš© WebSocket API
 */
import { ref, computed, onUnmounted } from 'vue'
import { createSTTOnlyStream } from '../services/voiceService'

export function useRealtimeSTT() {
  // ìƒíƒœ
  const isRecording = ref(false)
  const isConnected = ref(false)
  const isConnecting = ref(false)
  const error = ref(null)

  // ì¸ì‹ ê²°ê³¼
  const interimText = ref('')       // ì¤‘ê°„ ì¸ì‹ ê²°ê³¼ (recognizing)
  const finalTexts = ref([])        // ìµœì¢… ì¸ì‹ ê²°ê³¼ ë°°ì—´ (recognized)
  const recordingTime = ref(0)      // ë…¹ìŒ ì‹œê°„ (ì´ˆ)

  // ì˜¤ë””ì˜¤ ë…¹ìŒ (ë°œìŒ í‰ê°€ìš©)
  const audioBlob = ref(null)       // ë…¹ìŒëœ ì˜¤ë””ì˜¤ Blob

  // ë‚´ë¶€ ë¦¬ì†ŒìŠ¤
  let wsConnection = null
  let audioContext = null
  let audioWorkletNode = null
  let sourceNode = null
  let audioStream = null
  let recordingInterval = null
  let mediaRecorder = null          // MediaRecorder for audio capture
  let audioChunks = []              // ë…¹ìŒëœ ì˜¤ë””ì˜¤ ì²­í¬

  // ìë™ ëª¨ë“œìš© PCM ë²„í¼ (ì„¸ê·¸ë¨¼íŠ¸ë³„ ì˜¤ë””ì˜¤ ìº¡ì²˜)
  let pcmBuffer = []                // PCM ë°ì´í„° ë²„í¼ (Int16Array ì²­í¬ë“¤)
  let segmentPcmStart = 0           // í˜„ì¬ ì„¸ê·¸ë¨¼íŠ¸ ì‹œì‘ ìœ„ì¹˜
  let isNewSegmentStarted = false   // ìƒˆ ì„¸ê·¸ë¨¼íŠ¸ê°€ ì‹œì‘ë˜ì—ˆëŠ”ì§€ (onRecognizing ìµœì´ˆ ê°ì§€ìš©)

  // TTS ì—ì½” ë°©ì§€: TTS ì¬ìƒ ì¤‘ì—ëŠ” STT ê²°ê³¼ ë¬´ì‹œ
  let isTTSPlaying = false          // TTS ì¬ìƒ ì¤‘ í”Œë˜ê·¸ (ì™¸ë¶€ì—ì„œ ì„¤ì •)

  /**
   * ì „ì²´ ì¸ì‹ëœ í…ìŠ¤íŠ¸ (ìµœì¢… í…ìŠ¤íŠ¸ + ì¤‘ê°„ í…ìŠ¤íŠ¸)
   */
  const fullText = computed(() => {
    const finals = finalTexts.value.join(' ')
    const interim = interimText.value
    return interim ? `${finals} ${interim}`.trim() : finals
  })

  // ìë™ ë¶„ì ˆ ëª¨ë“œì—ì„œ recognized ì´ë²¤íŠ¸ ì½œë°± (ì™¸ë¶€ì—ì„œ ì„¤ì •)
  let onAutoRecognizedCallback = null

  /**
   * ìë™ ë¶„ì ˆ ëª¨ë“œì—ì„œ recognized ì´ë²¤íŠ¸ ì½œë°± ì„¤ì •
   * @param {Function} callback - ì½œë°± í•¨ìˆ˜ (text: string, audioBlob: Blob|null) => void
   */
  function setOnAutoRecognized(callback) {
    onAutoRecognizedCallback = callback
  }

  /**
   * TTS ì¬ìƒ ìƒíƒœ ì„¤ì • (ì—ì½” ë°©ì§€ìš©)
   * TTS ì¬ìƒ ì¤‘ì—ëŠ” STT ì¸ì‹ ê²°ê³¼ë¥¼ ë¬´ì‹œí•˜ì—¬ ì—ì½” ë°©ì§€
   * @param {boolean} isPlaying - TTS ì¬ìƒ ì¤‘ ì—¬ë¶€
   */
  function setTTSPlaying(isPlaying) {
    isTTSPlaying = isPlaying
    if (isPlaying) {
      // TTS ì‹œì‘ ì‹œ: í˜„ì¬ ì„¸ê·¸ë¨¼íŠ¸ ìƒíƒœ ë¦¬ì…‹
      isNewSegmentStarted = false
    } else {
      // TTS ì¢…ë£Œ ì‹œ: PCM ë²„í¼ ì‹œì‘ì ì„ í˜„ì¬ ìœ„ì¹˜ë¡œ ë¦¬ì…‹ (ì—ì½” ì˜¤ë””ì˜¤ ë²„ë¦¼)
      segmentPcmStart = pcmBuffer.length
      isNewSegmentStarted = false
    }
  }

  /**
   * PCM ë°ì´í„°ë¥¼ WAV Blobìœ¼ë¡œ ë³€í™˜
   * @param {Int16Array} pcmData - PCM ë°ì´í„° (16bit, 16kHz, mono)
   * @returns {Blob} WAV í˜•ì‹ Blob
   */
  function pcmToWav(pcmData) {
    const sampleRate = 16000
    const numChannels = 1
    const bitsPerSample = 16
    const byteRate = sampleRate * numChannels * bitsPerSample / 8
    const blockAlign = numChannels * bitsPerSample / 8
    const dataSize = pcmData.length * 2 // 16bit = 2 bytes per sample

    const buffer = new ArrayBuffer(44 + dataSize)
    const view = new DataView(buffer)

    // WAV í—¤ë” ì‘ì„±
    const writeString = (offset, string) => {
      for (let i = 0; i < string.length; i++) {
        view.setUint8(offset + i, string.charCodeAt(i))
      }
    }

    writeString(0, 'RIFF')
    view.setUint32(4, 36 + dataSize, true)
    writeString(8, 'WAVE')
    writeString(12, 'fmt ')
    view.setUint32(16, 16, true) // fmt chunk size
    view.setUint16(20, 1, true)  // PCM format
    view.setUint16(22, numChannels, true)
    view.setUint32(24, sampleRate, true)
    view.setUint32(28, byteRate, true)
    view.setUint16(32, blockAlign, true)
    view.setUint16(34, bitsPerSample, true)
    writeString(36, 'data')
    view.setUint32(40, dataSize, true)

    // PCM ë°ì´í„° ë³µì‚¬
    const pcmView = new Int16Array(buffer, 44)
    pcmView.set(pcmData)

    return new Blob([buffer], { type: 'audio/wav' })
  }

  /**
   * í˜„ì¬ ì„¸ê·¸ë¨¼íŠ¸ì˜ PCM ë°ì´í„°ë¥¼ WAVë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜
   * @returns {Blob|null} WAV Blob ë˜ëŠ” null
   */
  function captureSegmentAudio() {
    // í˜„ì¬ ì„¸ê·¸ë¨¼íŠ¸ì˜ ë ìœ„ì¹˜ë¥¼ ë¨¼ì € ìº¡ì²˜ (í˜¸ì¶œ ì‹œì  ê¸°ì¤€)
    const segmentPcmEnd = pcmBuffer.length

    // í˜„ì¬ ì„¸ê·¸ë¨¼íŠ¸ì˜ PCM ë°ì´í„°ë§Œ ì¶”ì¶œ (start ~ end)
    const segmentPcmChunks = pcmBuffer.slice(segmentPcmStart, segmentPcmEnd)

    if (segmentPcmChunks.length === 0) {
      return null
    }

    // ëª¨ë“  ì²­í¬ë¥¼ í•˜ë‚˜ì˜ Int16Arrayë¡œ í•©ì¹˜ê¸°
    const totalLength = segmentPcmChunks.reduce((sum, chunk) => sum + chunk.length, 0)
    const combinedPcm = new Int16Array(totalLength)
    let offset = 0
    for (const chunk of segmentPcmChunks) {
      combinedPcm.set(chunk, offset)
      offset += chunk.length
    }

    // WAVë¡œ ë³€í™˜
    return pcmToWav(combinedPcm)
  }

  /**
   * ì‹¤ì‹œê°„ STT ë…¹ìŒ ì‹œì‘
   *
   * @param {string} language - ì¸ì‹ ì–¸ì–´ (BCP-47 ì½”ë“œ, ì˜ˆ: 'en-US')
   * @param {Object} options - ì¶”ê°€ ì˜µì…˜
   * @param {boolean} options.autoSegment - ìë™ ë¶„ì ˆ ëª¨ë“œ (ê¸°ë³¸: false, trueë©´ ì¹¨ë¬µ ê°ì§€ë¡œ ìë™ ë¬¸ì¥ ë¶„ë¦¬)
   */
  async function startRecording(language = 'en-US', options = {}) {
    const autoSegment = options.autoSegment || false
    if (isRecording.value || isConnecting.value) {
      console.warn('âš ï¸ Already recording or connecting')
      return
    }

    try {
      isConnecting.value = true
      error.value = null
      interimText.value = ''
      finalTexts.value = []
      recordingTime.value = 0
      audioBlob.value = null
      audioChunks = []
      pcmBuffer = []
      segmentPcmStart = 0
      isNewSegmentStarted = false

      // 1. ë§ˆì´í¬ ê¶Œí•œ ìš”ì²­
      audioStream = await navigator.mediaDevices.getUserMedia({
        audio: {
          sampleRate: 48000,
          channelCount: 1,
          echoCancellation: true,
          noiseSuppression: true
        }
      })

      // 1.5 MediaRecorder ì„¤ì • (ë°œìŒ í‰ê°€ìš© ì˜¤ë””ì˜¤ ìº¡ì²˜)
      try {
        const mimeType = MediaRecorder.isTypeSupported('audio/webm;codecs=opus')
          ? 'audio/webm;codecs=opus'
          : 'audio/webm'

        mediaRecorder = new MediaRecorder(audioStream, { mimeType })

        mediaRecorder.ondataavailable = (event) => {
          if (event.data.size > 0) {
            audioChunks.push(event.data)
          }
        }

        mediaRecorder.onstop = () => {
          if (audioChunks.length > 0) {
            audioBlob.value = new Blob(audioChunks, { type: 'audio/webm' })
            console.log('ğŸ¤ Audio captured:', audioBlob.value.size, 'bytes')
          }
        }

        mediaRecorder.start(100) // 100ms ê°„ê²©ìœ¼ë¡œ ë°ì´í„° ìˆ˜ì§‘
        console.log('ğŸ™ï¸ MediaRecorder started for pronunciation assessment')
      } catch (recorderErr) {
        console.warn('âš ï¸ MediaRecorder not available:', recorderErr)
        // MediaRecorder ì‹¤íŒ¨í•´ë„ STTëŠ” ê³„ì† ì§„í–‰
      }

      // 2. AudioContext ìƒì„±
      audioContext = new (window.AudioContext || window.webkitAudioContext)()

      // Chrome Autoplay Policy ëŒ€ì‘: suspended ìƒíƒœë©´ resume í•„ìš”
      if (audioContext.state === 'suspended') {
        await audioContext.resume()
      }

      // 3. AudioWorklet ë¡œë“œ (PCM ë³€í™˜ìš©)
      await audioContext.audioWorklet.addModule('/pcm-processor.js')

      // 4. AudioWorkletNode ìƒì„±
      audioWorkletNode = new AudioWorkletNode(audioContext, 'pcm-processor')

      // 5. ë§ˆì´í¬ ì—°ê²°
      sourceNode = audioContext.createMediaStreamSource(audioStream)
      sourceNode.connect(audioWorkletNode)

      // 6. WebSocket ì—°ê²° (ë‹¨ì¼ ì–¸ì–´ STT ì „ìš© - ë²ˆì—­ ì—†ìŒ)
      wsConnection = createSTTOnlyStream(language, {
        onConnected: () => {
          console.log(`âœ… [STT-Only] Realtime STT connected (autoSegment: ${autoSegment})`)

          setTimeout(() => {
            if (wsConnection && wsConnection.ws.readyState === WebSocket.OPEN) {
              // PCM ë°ì´í„° ì „ì†¡ ì‹œì‘ + ìë™ ëª¨ë“œìš© ë²„í¼ ì €ì¥
              audioWorkletNode.port.onmessage = (event) => {
                if (wsConnection && wsConnection.ws.readyState === WebSocket.OPEN) {
                  wsConnection.ws.send(event.data)

                  // ìë™ ëª¨ë“œ: PCM ë°ì´í„°ë¥¼ ë²„í¼ì— ì €ì¥ (ë°œìŒ í‰ê°€ìš©)
                  if (autoSegment) {
                    // ArrayBufferë¥¼ Int16Arrayë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥
                    const pcmChunk = new Int16Array(event.data)
                    pcmBuffer.push(pcmChunk)
                  }
                }
              }

              isRecording.value = true
              isConnected.value = true
              isConnecting.value = false

              // ë…¹ìŒ ì‹œê°„ íƒ€ì´ë¨¸ ì‹œì‘
              recordingInterval = setInterval(() => {
                recordingTime.value++
              }, 1000)
            } else {
              error.value = 'WebSocket ì—°ê²°ì´ ë¶ˆì•ˆì •í•©ë‹ˆë‹¤'
              isConnecting.value = false
              cleanup()
            }
          }, 200)
        },

        onRecognizing: (message) => {
          // ì¤‘ê°„ ì¸ì‹ ê²°ê³¼
          interimText.value = message.text || ''

          // ìë™ ë¶„ì ˆ ëª¨ë“œ: ìƒˆ ì„¸ê·¸ë¨¼íŠ¸ ì‹œì‘ ê°ì§€
          if (autoSegment && !isNewSegmentStarted) {
            isNewSegmentStarted = true
            // í˜„ì¬ ë²„í¼ ìœ„ì¹˜ì—ì„œ ì•½ 1.5ì´ˆ(24ì²­í¬) ì „ìœ¼ë¡œ ì‹œì‘ì  ì„¤ì •
            const backtrackChunks = 24
            segmentPcmStart = Math.max(segmentPcmStart, pcmBuffer.length - backtrackChunks)
          }
        },

        onRecognized: (message) => {
          // ìµœì¢… ì¸ì‹ ê²°ê³¼
          const text = message.text?.trim()
          if (text) {
            // TTS ì¬ìƒ ì¤‘ì—ëŠ” ì—ì½” ë°©ì§€ë¥¼ ìœ„í•´ ê²°ê³¼ ë¬´ì‹œ
            if (isTTSPlaying) {
              segmentPcmStart = pcmBuffer.length
              isNewSegmentStarted = false
              return
            }

            finalTexts.value.push(text)
            interimText.value = ''

            // ìë™ ë¶„ì ˆ ëª¨ë“œì—ì„œ ì½œë°± í˜¸ì¶œ (ë©”ì‹œì§€ ìë™ ì „ì†¡ìš©)
            if (autoSegment && onAutoRecognizedCallback) {
              const segmentAudioBlob = captureSegmentAudio()
              onAutoRecognizedCallback(text, segmentAudioBlob)

              // ë‹¤ìŒ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ìœ„í•´ ìƒíƒœ ë¦¬ì…‹
              segmentPcmStart = pcmBuffer.length
              isNewSegmentStarted = false
            }
          }
        },

        onError: (errorMessage) => {
          console.error('âŒ Realtime STT error:', errorMessage)
          error.value = errorMessage
        },

        onEnd: () => {
          console.log('ğŸ”š Realtime STT ended')
          isConnected.value = false
        }
      }, { autoSegment })

    } catch (err) {
      console.error('âŒ Failed to start realtime STT:', err)
      error.value = err.message || 'ë…¹ìŒ ì‹œì‘ ì‹¤íŒ¨'
      isConnecting.value = false
      cleanup()
      throw err
    }
  }

  /**
   * ë…¹ìŒ ì¤‘ì§€ ë° ê²°ê³¼ ë°˜í™˜
   *
   * @returns {Object} { text: string, audioBlob: Blob|null }
   */
  function stopRecording() {
    // íƒ€ì´ë¨¸ ì¤‘ì§€
    if (recordingInterval) {
      clearInterval(recordingInterval)
      recordingInterval = null
    }

    // ìµœì¢… í…ìŠ¤íŠ¸ ìº¡ì²˜ (ì¤‘ì§€ ì „ì—)
    const resultText = fullText.value

    // MediaRecorder ì¤‘ì§€ (ì˜¤ë””ì˜¤ ìº¡ì²˜ ì™„ë£Œ)
    let capturedAudioBlob = null
    if (mediaRecorder && mediaRecorder.state !== 'inactive') {
      mediaRecorder.stop()
      // ë™ê¸°ì ìœ¼ë¡œ Blob ìƒì„± (onstop ëŒ€ê¸°í•˜ì§€ ì•ŠìŒ)
      if (audioChunks.length > 0) {
        capturedAudioBlob = new Blob(audioChunks, { type: 'audio/webm' })
        audioBlob.value = capturedAudioBlob
        console.log('ğŸ¤ Audio captured immediately:', capturedAudioBlob.size, 'bytes')
      }
    }

    // ë¦¬ì†ŒìŠ¤ ì •ë¦¬
    cleanup()

    console.log('â¹ï¸ Recording stopped, text:', resultText)
    return {
      text: resultText,
      audioBlob: capturedAudioBlob
    }
  }

  /**
   * ë¦¬ì†ŒìŠ¤ ì •ë¦¬
   */
  function cleanup() {
    try {
      // MediaRecorder ì •ë¦¬
      if (mediaRecorder && mediaRecorder.state !== 'inactive') {
        try {
          mediaRecorder.stop()
        } catch (e) {
          // ì´ë¯¸ ì¤‘ì§€ë¨
        }
      }
      mediaRecorder = null

      // Audio ë…¸ë“œ ì •ë¦¬
      if (sourceNode) {
        sourceNode.disconnect()
        sourceNode = null
      }

      if (audioWorkletNode) {
        audioWorkletNode.disconnect()
        audioWorkletNode.port.onmessage = null
        audioWorkletNode = null
      }

      if (audioContext && audioContext.state !== 'closed') {
        audioContext.close()
        audioContext = null
      }

      if (audioStream) {
        audioStream.getTracks().forEach(track => track.stop())
        audioStream = null
      }

      // WebSocket ì •ë¦¬
      if (wsConnection) {
        wsConnection.close()
        wsConnection = null
      }

      // ìƒíƒœ ì´ˆê¸°í™”
      isRecording.value = false
      isConnected.value = false
      isConnecting.value = false

    } catch (err) {
      console.error('âŒ Cleanup error:', err)
    }
  }

  /**
   * ê²°ê³¼ ì´ˆê¸°í™”
   * ì£¼ì˜: audioChunksëŠ” ì—¬ê¸°ì„œ ë¹„ìš°ì§€ ì•ŠìŒ (ë…¹ìŒì´ ê³„ì† ì§„í–‰ ì¤‘ì¼ ìˆ˜ ìˆìŒ)
   * ìë™ ëª¨ë“œì—ì„œëŠ” captureSegmentAudio()ê°€ ì²­í¬ë¥¼ ê´€ë¦¬í•¨
   */
  function clearResults() {
    interimText.value = ''
    finalTexts.value = []
    recordingTime.value = 0
    error.value = null
    audioBlob.value = null
    // audioChunksëŠ” ë¹„ìš°ì§€ ì•ŠìŒ - ë…¹ìŒì´ ê³„ì† ì§„í–‰ ì¤‘ì¼ ìˆ˜ ìˆìŒ
  }

  // ì»´í¬ë„ŒíŠ¸ ì–¸ë§ˆìš´íŠ¸ ì‹œ ì •ë¦¬
  onUnmounted(() => {
    if (recordingInterval) {
      clearInterval(recordingInterval)
    }
    cleanup()
  })

  return {
    // ìƒíƒœ
    isRecording,
    isConnected,
    isConnecting,
    error,

    // ì¸ì‹ ê²°ê³¼
    interimText,
    finalTexts,
    fullText,
    recordingTime,
    audioBlob,  // ë…¹ìŒëœ ì˜¤ë””ì˜¤ Blob (ë°œìŒ í‰ê°€ìš©)

    // ë©”ì„œë“œ
    startRecording,
    stopRecording,
    clearResults,
    setOnAutoRecognized,  // ìë™ ë¶„ì ˆ ëª¨ë“œ ì½œë°± ì„¤ì •
    setTTSPlaying         // TTS ì¬ìƒ ìƒíƒœ ì„¤ì • (ì—ì½” ë°©ì§€ìš©)
  }
}
