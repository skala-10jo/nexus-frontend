/**
 * WebSocket STT Composable (ë°±ì—”ë“œ ê²½ìœ  ë°©ì‹)
 *
 * ë°±ì—”ë“œ WebSocket ê¸°ë°˜ êµ¬ì¡°:
 * - Frontend: ë§ˆì´í¬ ì˜¤ë””ì˜¤ë¥¼ WebSocketìœ¼ë¡œ Backend ì „ì†¡
 * - Backend: Azure Speech SDKë¡œ STT ìˆ˜í–‰ í›„ ê²°ê³¼ ë°˜í™˜
 * - Frontend: ì¸ì‹ ê²°ê³¼ ë°›ì•„ì„œ ë²ˆì—­ ìš”ì²­
 *
 * ì˜¤ë””ì˜¤ ì²˜ë¦¬:
 * - Web Audio APIë¡œ ë§ˆì´í¬ ì…ë ¥ ìº¡ì²˜
 * - PCM 16kHz monoë¡œ ë³€í™˜
 * - Base64 ì¸ì½”ë”©í•˜ì—¬ WebSocket ì „ì†¡
 *
 * ì¸ì¦:
 * - JWT í† í°ì„ WebSocket ì—°ê²° ì‹œ ì „ì†¡
 * - Backendì—ì„œ í† í° ê²€ì¦ í›„ ì—°ê²° í—ˆìš©
 *
 * ë‹¤êµ­ì–´ ì§€ì›:
 * - Azure Speech SDK ìë™ ì–¸ì–´ ê°ì§€
 * - ì¸ì‹ëœ ì–¸ì–´ë¡œë¶€í„° ë‹¤ë¥¸ ì–¸ì–´ë¡œ ë²ˆì—­ (Backend Agent)
 *
 * @example
 * import { useWebSocketSTT } from '@/composables/useWebSocketSTT'
 *
 * const {
 *   isRecording,
 *   translationCards,
 *   startRecording,
 *   stopRecording
 * } = useWebSocketSTT()
 *
 * // ë‹¤êµ­ì–´ STT + ë²ˆì—­ ì‹œì‘
 * await startRecording(['ko-KR', 'en-US', 'ja-JP', 'vi-VN'])
 */
import { ref, onUnmounted } from 'vue'
import { pythonAPI } from '@/services/api'

export function useWebSocketSTT() {
  // ìƒíƒœ
  const isRecording = ref(false)
  const isConnected = ref(false)
  const error = ref(null)

  // ë²ˆì—­ ì¹´ë“œ ëª©ë¡
  const translationCards = ref([])
  const recognizingText = ref('')

  // WebSocket ë° ì˜¤ë””ì˜¤ ë¦¬ì†ŒìŠ¤
  let ws = null
  let audioContext = null
  let mediaStream = null
  let scriptProcessor = null
  let selectedLanguages = []

  /**
   * ë…¹ìŒ ì‹œì‘ (WebSocket + Backend Azure Speech SDK)
   *
   * @param {string[]} languages - ì„ íƒëœ ì–¸ì–´ ëª©ë¡ (BCP-47)
   *   ì˜ˆ: ['ko-KR', 'en-US', 'ja-JP', 'vi-VN']
   */
  async function startRecording(languages = ['ko-KR', 'en-US']) {
    try {
      // ì´ì „ ì„¸ì…˜ ì •ë¦¬
      if (ws || audioContext) {
        console.log('âš ï¸ Cleaning up previous session...')
        cleanup()
      }

      error.value = null
      selectedLanguages = languages

      // ì–¸ì–´ ê²€ì¦
      if (!languages || languages.length < 2) {
        throw new Error('ìµœì†Œ 2ê°œ ì´ìƒì˜ ì–¸ì–´ë¥¼ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤')
      }

      console.log('ğŸ¤ Starting WebSocket STT...')
      console.log('ğŸ“‹ Selected languages:', languages)

      // JWT í† í° ê°€ì ¸ì˜¤ê¸°
      const token = localStorage.getItem('token')
      if (!token) {
        throw new Error('ë¡œê·¸ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤')
      }

      // 1. WebSocket ì—°ê²°
      await connectWebSocket(token, languages)

      // 2. ë§ˆì´í¬ ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ì‹œì‘
      await startAudioCapture()

      console.log('âœ… Recording started')
    } catch (err) {
      console.error('âŒ Failed to start recording:', err)
      error.value = err.message || 'ë…¹ìŒ ì‹œì‘ ì‹¤íŒ¨'
      cleanup()
      throw err
    }
  }

  /**
   * WebSocket ì—°ê²° ë° ì¸ì¦
   */
  async function connectWebSocket(token, languages) {
    return new Promise((resolve, reject) => {
      try {
        // WebSocket URL ìƒì„±
        const wsProtocol = window.location.protocol === 'https:' ? 'wss:' : 'ws:'
        const wsHost = import.meta.env.VITE_PYTHON_API_URL?.replace(/^https?:\/\//, '') || 'localhost:8000'
        const wsUrl = `${wsProtocol}//${wsHost}/voice/ws/stt`

        console.log('ğŸ”Œ Connecting to WebSocket:', wsUrl)

        ws = new WebSocket(wsUrl)

        // ì—°ê²° ì„±ê³µ
        ws.onopen = async () => {
          console.log('âœ… WebSocket connected')

          try {
            // 1. ì¸ì¦ ë©”ì‹œì§€ ì „ì†¡
            ws.send(JSON.stringify({
              type: 'auth',
              token: token
            }))

            console.log('ğŸ”‘ Auth message sent')

            // ì¸ì¦ ì‘ë‹µ ëŒ€ê¸° (íƒ€ì„ì•„ì›ƒ 10ì´ˆ)
            const authTimeout = setTimeout(() => {
              reject(new Error('ì¸ì¦ íƒ€ì„ì•„ì›ƒ'))
            }, 10000)

            // ì¼íšŒì„± ë©”ì‹œì§€ í•¸ë“¤ëŸ¬ (ì¸ì¦ ì‘ë‹µ ëŒ€ê¸°)
            const waitForAuth = (event) => {
              const data = JSON.parse(event.data)

              if (data.type === 'auth_success') {
                clearTimeout(authTimeout)
                console.log('âœ… Authentication successful:', data.username)

                // 2. ì„¤ì • ë©”ì‹œì§€ ì „ì†¡ (ì–¸ì–´, diarization)
                const primaryLanguage = languages[0] // ì²« ë²ˆì§¸ ì–¸ì–´ë¥¼ primaryë¡œ
                ws.send(JSON.stringify({
                  type: 'config',
                  language: primaryLanguage,
                  enable_diarization: false
                }))

                console.log('âš™ï¸ Config sent:', { language: primaryLanguage })

                isConnected.value = true

                // ì¼íšŒì„± í•¸ë“¤ëŸ¬ ì œê±°í•˜ê³  ì‹¤ì œ ë©”ì‹œì§€ í•¸ë“¤ëŸ¬ ë“±ë¡
                ws.removeEventListener('message', waitForAuth)
                ws.addEventListener('message', handleWebSocketMessage)

                resolve()
              } else if (data.type === 'auth_error') {
                clearTimeout(authTimeout)
                reject(new Error(data.message || 'ì¸ì¦ ì‹¤íŒ¨'))
              }
            }

            ws.addEventListener('message', waitForAuth)
          } catch (err) {
            reject(err)
          }
        }

        // ì—°ê²° ì˜¤ë¥˜
        ws.onerror = (event) => {
          console.error('âŒ WebSocket error:', event)
          error.value = 'WebSocket ì—°ê²° ì˜¤ë¥˜'
          reject(new Error('WebSocket ì—°ê²° ì‹¤íŒ¨'))
        }

        // ì—°ê²° ì¢…ë£Œ
        ws.onclose = (event) => {
          console.log('ğŸ”´ WebSocket closed:', event.code, event.reason)
          isConnected.value = false

          if (isRecording.value) {
            // ì˜ˆìƒì¹˜ ëª»í•œ ì¢…ë£Œ
            error.value = 'WebSocket ì—°ê²°ì´ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤'
            cleanup()
          }
        }
      } catch (err) {
        reject(err)
      }
    })
  }

  /**
   * WebSocket ë©”ì‹œì§€ í•¸ë“¤ëŸ¬
   */
  function handleWebSocketMessage(event) {
    try {
      const data = JSON.parse(event.data)

      switch (data.type) {
        case 'interim':
          // ì¤‘ê°„ ì¸ì‹ ê²°ê³¼
          recognizingText.value = data.text || ''
          console.log('ğŸ” Interim:', data.text)
          break

        case 'final':
          // ìµœì¢… ì¸ì‹ ê²°ê³¼
          handleFinalRecognition(data)
          break

        case 'error':
          // ì—ëŸ¬
          console.error('âŒ Backend error:', data.message)
          error.value = data.message
          break

        case 'session_stopped':
          // ì„¸ì…˜ ì¢…ë£Œ
          console.log('ğŸ”´ Session stopped')
          cleanup()
          break

        case 'closed':
          // WebSocket ì¢…ë£Œ
          console.log('ğŸ”´ WebSocket closed by server')
          cleanup()
          break

        default:
          console.warn('âš ï¸ Unknown message type:', data.type)
      }
    } catch (err) {
      console.error('âŒ Failed to parse WebSocket message:', err)
    }
  }

  /**
   * ìµœì¢… ì¸ì‹ ê²°ê³¼ ì²˜ë¦¬ ë° ë²ˆì—­
   */
  async function handleFinalRecognition(data) {
    const recognizedText = data.text
    const detectedLanguage = data.language || selectedLanguages[0] // Backendì—ì„œ ì–¸ì–´ ê°ì§€ ì •ë³´ ì œê³µ
    const confidence = data.confidence

    console.log('âœ… Final recognition:', {
      text: recognizedText,
      language: detectedLanguage,
      confidence: confidence
    })

    recognizingText.value = ''

    if (recognizedText && recognizedText.trim()) {
      // ë°±ì—”ë“œë¡œ ë²ˆì—­ ìš”ì²­
      try {
        const translations = await translateToMultipleLanguages(
          recognizedText,
          detectedLanguage,
          selectedLanguages
        )

        // ë²ˆì—­ ì¹´ë“œ ì¶”ê°€
        translationCards.value.unshift({
          id: Date.now(),
          original: recognizedText,
          detectedLang: detectedLanguage,
          translations: translations,
          timestamp: new Date().toISOString(),
          confidence: confidence
        })

        // ìµœëŒ€ 50ê°œ ì¹´ë“œë§Œ ìœ ì§€
        if (translationCards.value.length > 50) {
          translationCards.value = translationCards.value.slice(0, 50)
        }

        console.log('ğŸ“ Translation card added')
      } catch (err) {
        console.error('âŒ Translation failed:', err)
        error.value = 'ë²ˆì—­ ì‹¤íŒ¨: ' + err.message
      }
    }
  }

  /**
   * ë‹¤êµ­ì–´ ë²ˆì—­ (ë°±ì—”ë“œ TranslationAgent í˜¸ì¶œ)
   *
   * @param {string} text - ì¸ì‹ëœ ì›ë¬¸
   * @param {string} sourceLang - ê°ì§€ëœ ì–¸ì–´ (BCP-47: ko-KR, en-US ë“±)
   * @param {string[]} selectedLanguages - ì „ì²´ ì„ íƒ ì–¸ì–´ ëª©ë¡
   * @returns {Promise<Array>} ë²ˆì—­ ê²°ê³¼ ë°°ì—´
   */
  async function translateToMultipleLanguages(text, sourceLang, selectedLanguages) {
    // BCP-47 â†’ ISO 639-1 ë³€í™˜ (ko-KR â†’ ko, en-US â†’ en)
    const sourceIso = sourceLang.split('-')[0]

    // ì›ë³¸ ì–¸ì–´ ì œì™¸í•œ ëª©í‘œ ì–¸ì–´ ì¶”ì¶œ
    const targetLanguages = selectedLanguages
      .filter(lang => !lang.startsWith(sourceIso))
      .map(lang => lang.split('-')[0])

    if (targetLanguages.length === 0) {
      return []
    }

    console.log('ğŸ”„ Translating:', {
      text,
      from: sourceIso,
      to: targetLanguages
    })

    // ë°±ì—”ë“œ ë²ˆì—­ API í˜¸ì¶œ
    const response = await pythonAPI.post('/translate/batch', {
      texts: [text],
      source_lang: sourceIso,
      target_lang: targetLanguages[0] // ì²« ë²ˆì§¸ ëª©í‘œ ì–¸ì–´
    })

    // ì—¬ëŸ¬ ì–¸ì–´ë¡œ ë²ˆì—­ì´ í•„ìš”í•œ ê²½ìš° ìˆœì°¨ í˜¸ì¶œ
    const allTranslations = []
    for (const targetLang of targetLanguages) {
      const res = await pythonAPI.post('/translate', {
        text: text,
        source_lang: sourceIso,
        target_lang: targetLang
      })

      if (res.data.success) {
        allTranslations.push({
          lang: targetLang,
          text: res.data.data.translated_text
        })
      }
    }

    return allTranslations
  }

  /**
   * ë§ˆì´í¬ ì˜¤ë””ì˜¤ ìº¡ì²˜ ì‹œì‘
   */
  async function startAudioCapture() {
    try {
      // ë§ˆì´í¬ ê¶Œí•œ ìš”ì²­
      mediaStream = await navigator.mediaDevices.getUserMedia({
        audio: {
          sampleRate: 16000, // Azure Speech SDK ìš”êµ¬ì‚¬í•­
          channelCount: 1,   // Mono
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true
        }
      })

      console.log('ğŸ™ï¸ Microphone access granted')

      // Web Audio API ì´ˆê¸°í™”
      audioContext = new (window.AudioContext || window.webkitAudioContext)({
        sampleRate: 16000
      })

      // Chrome Autoplay Policy ëŒ€ì‘: suspended ìƒíƒœë©´ resume í•„ìš”
      if (audioContext.state === 'suspended') {
        await audioContext.resume()
      }

      const source = audioContext.createMediaStreamSource(mediaStream)

      // ScriptProcessor (ì˜¤ë””ì˜¤ ì²˜ë¦¬)
      const bufferSize = 4096
      scriptProcessor = audioContext.createScriptProcessor(bufferSize, 1, 1)

      scriptProcessor.onaudioprocess = (event) => {
        if (!isRecording.value || !ws || ws.readyState !== WebSocket.OPEN) {
          return
        }

        // PCM ì˜¤ë””ì˜¤ ë°ì´í„° ì¶”ì¶œ
        const inputData = event.inputBuffer.getChannelData(0)

        // Float32 â†’ Int16 ë³€í™˜
        const int16Data = new Int16Array(inputData.length)
        for (let i = 0; i < inputData.length; i++) {
          const s = Math.max(-1, Math.min(1, inputData[i]))
          int16Data[i] = s < 0 ? s * 0x8000 : s * 0x7FFF
        }

        // Base64 ì¸ì½”ë”©
        const base64Audio = arrayBufferToBase64(int16Data.buffer)

        // WebSocketìœ¼ë¡œ ì „ì†¡
        ws.send(JSON.stringify({
          type: 'audio',
          data: base64Audio
        }))
      }

      // ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸ ì—°ê²°
      source.connect(scriptProcessor)
      scriptProcessor.connect(audioContext.destination)

      isRecording.value = true
      console.log('ğŸ™ï¸ Audio capture started')
    } catch (err) {
      console.error('âŒ Failed to start audio capture:', err)
      throw new Error('ë§ˆì´í¬ ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤')
    }
  }

  /**
   * ArrayBufferë¥¼ Base64ë¡œ ì¸ì½”ë”©
   */
  function arrayBufferToBase64(buffer) {
    let binary = ''
    const bytes = new Uint8Array(buffer)
    const len = bytes.byteLength
    for (let i = 0; i < len; i++) {
      binary += String.fromCharCode(bytes[i])
    }
    return window.btoa(binary)
  }

  /**
   * ë…¹ìŒ ì¤‘ì§€
   */
  function stopRecording() {
    try {
      console.log('â¹ï¸ Stopping recording...')

      // WebSocketì— ì¤‘ì§€ ì‹ í˜¸ ì „ì†¡
      if (ws && ws.readyState === WebSocket.OPEN) {
        ws.send(JSON.stringify({ type: 'stop' }))
        console.log('ğŸ“¤ Stop signal sent')
      }

      cleanup()
      console.log('âœ… Recording stopped')
    } catch (err) {
      console.error('âŒ Failed to stop recording:', err)
      error.value = err.message || 'ë…¹ìŒ ì¤‘ì§€ ì‹¤íŒ¨'
      cleanup()
    }
  }

  /**
   * ë²ˆì—­ ì¹´ë“œ ì´ˆê¸°í™”
   */
  function clearCards() {
    translationCards.value = []
    console.log('ğŸ—‘ï¸ Translation cards cleared')
  }

  /**
   * ë¦¬ì†ŒìŠ¤ ì •ë¦¬
   */
  function cleanup() {
    // WebSocket ì •ë¦¬
    if (ws) {
      try {
        if (ws.readyState === WebSocket.OPEN || ws.readyState === WebSocket.CONNECTING) {
          ws.close()
        }
      } catch (err) {
        console.error('âŒ Failed to close WebSocket:', err)
      }
      ws = null
    }

    // ì˜¤ë””ì˜¤ ì •ë¦¬
    if (scriptProcessor) {
      scriptProcessor.disconnect()
      scriptProcessor = null
    }

    if (mediaStream) {
      mediaStream.getTracks().forEach(track => track.stop())
      mediaStream = null
    }

    if (audioContext) {
      audioContext.close()
      audioContext = null
    }

    isRecording.value = false
    isConnected.value = false
    recognizingText.value = ''

    console.log('ğŸ§¹ Resources cleaned up')
  }

  // ì»´í¬ë„ŒíŠ¸ ì–¸ë§ˆìš´íŠ¸ ì‹œ ìë™ ì •ë¦¬
  onUnmounted(() => {
    cleanup()
  })

  return {
    // ìƒíƒœ
    isRecording,
    isConnected,
    error,

    // ë°ì´í„°
    translationCards,
    recognizingText,

    // ë©”ì„œë“œ
    startRecording,
    stopRecording,
    clearCards
  }
}
